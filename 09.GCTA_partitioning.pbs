#!/bin/bash
#PBS -q workq
#PBS -N GREML_pa6
#PBS -V 
#PBS -S /bin/bash 
#PBS -l nodes=1:ppn=10
#PBS -o GREML_part6.log
#PBS -e GREML_part6.err

#input 
plink=/home/edizadehm/01.software/02.plink/plink
gcta=/home/edizadehm/02.tutorials/07.gcta/gcta-1.94.1-linux-kernel-3-x86_64/gcta64
input=/home/edizadehm/03.resources/03.SCZ_CC/03.staging/SCZ_CC_3465_het_filtered
r_part=/home/edizadehm/03.resources/03.SCZ_CC/01.scripts/10.partitioning.R
py_part=/home/edizadehm/03.resources/03.SCZ_CC/01.scripts/10.partitioning.py
hapmap3_snps=/home/edizadehm/03.resources/02.hapmap/02.hapmap3_snps/hapmap3_rs_only.txt
thread=10
#outputs
output_dir=/home/edizadehm/03.resources/03.SCZ_CC/03.staging/
base_name=SCZ_CC_GCTA_part
duplicate_snps=${output_dir}duplicate_snps.txt
base=${output_dir}${base_name}
input_dup_removed=${base}_dup_removed
ldscore=${base}_2
grm_base=${base}_base
grm_filtered=${base}_filtered
pca_20=${base}_PCA_20
phen=/home/edizadehm/03.resources/03.SCZ_CC/03.staging/SCZ_CC_GCTA.phen
pca20=/home/edizadehm/03.resources/03.SCZ_CC/03.staging/SCZ_CC_GCTA_part_PCA_20.eigenvec
output=${base}.final
multiple_grm=/home/edizadehm/03.resources/03.SCZ_CC/03.staging/multiple_grm.txt
greml=${base}.result


#Remove the duplicate SNPs, unless we gonna get error
# awk '{print $2}' ${input}.bim | sort | uniq -d > $duplicate_snps
# $plink --bfile $input --exclude $duplicate_snps --allow-no-sex --make-bed --out $input_dup_removed
# echo -e "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n"

# make grm
allele_freq=$(echo "scale=10; 5/(3465 * 2)" | bc)
#calcualted according to the paper: 5 Allele counts: 5 / n * 2. n is sample size
# $gcta --bfile $input_dup_removed --autosome --maf 0.0007 --make-grm --out $grm_base --thread-num 10 
echo -e "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n"
#removing cryptic relatedness 
#remove one of each pair of individuals with estimated genetic relatedness > 0.05, According to the paper.
# $gcta --grm $grm_base --grm-cutoff 0.05  --make-grm  --out $grm_filtered
#these are the final family and individuals that are going to be included in the study.
echo -e "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n"

#Run GREML for entire grm, common and rare together
$gcta --reml --reml-alg 0 --grm $grm_filtered --qcovar $pca20 --pheno $phen --prevalence 0.01 --out ${greml}_all --thread-num $thread --reml-maxit 10000

#in GREML we'll need quantitative covariants, therfore, we are going to calcualte PCA for our samples.
#The PCA must be acording to HapMao snps and includes 20 PCAs as covariates later in the analysis.
# Input the GRM file and output the first 20 eigenvectors for a subset of individuals
# $gcta --grm $grm_filtered --keep ${grm_filtered}.grm.id --extract $hapmap3_snps --pca 20  --out $pca_20
echo -e "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n"

#calculate LD score
# --ld-score-region 200
#The default value is 200Kb, i.e. the length of the segment is 200Kb (with 100Kb overlap between two adjacent segments). Results are save a *.score.ld file.
#In the paper, the used 10MB for ld_score window.
# $gcta --bfile $input_dup_removed  --keep ${grm_filtered}.grm.id --ld-score-region 10000 --out $ldscore --thread-num $thread

# partitio  by LD
ld_out=${ldscore}.score.ld
# Rscript $r_part $ld_out
# python3 $py_part $ld_out

# #Making GRM for each LD group and further stratify by MAF
# $gcta --bfile $input_dup_removed  --keep ${grm_filtered}.grm.id --extract ${ld_out}.high_ld.txt --maf 0.01 --thread-num $thread --make-grm --out ${grm_base}.high_common
echo -e "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n"
# $gcta --bfile $input_dup_removed  --keep ${grm_filtered}.grm.id --extract ${ld_out}.high_ld.txt --max-maf 0.01 --thread-num $thread --make-grm --out ${grm_base}.high_rare
echo -e "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n"
# # #g2
# $gcta --bfile $input_dup_removed  --keep ${grm_filtered}.grm.id --extract ${ld_out}.low_ld.txt --maf 0.01 --thread-num $thread --make-grm --out ${grm_base}.low_common
echo -e "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n"
# $gcta --bfile $input_dup_removed  --keep ${grm_filtered}.grm.id --extract ${ld_out}.low_ld.txt --max-maf 0.01 --thread-num $thread --make-grm --out ${grm_base}.low_rare
echo -e "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n"
# # #g3
# $gcta --bfile $input --extract ${ld_out}.snp_group3.txt --maf 0.1 --thread-num $thread --make-grm --out ${grm_base}.g3_maf_mt_0.1
# $gcta --bfile $input --extract ${ld_out}.snp_group3.txt --max-maf 0.1 --thread-num $thread --make-grm --out ${grm_base}.g3_maf_lt_0.1
# # #g4
# $gcta --bfile $input --extract ${ld_out}.snp_group4.txt --maf 0.1 --thread-num $thread --make-grm --out ${grm_base}.g4_maf_mt_0.1
# $gcta --bfile $input --extract ${ld_out}.snp_group4.txt --max-maf 0.1 --thread-num $thread --make-grm --out ${grm_base}.g4_maf_lt_0.1

# #make multiple grm file
# echo -e ${grm_base}.g1_maf_mt_0.1\n${grm_base}.g1_maf_lt_0.1\n${grm_base}.g2_maf_mt_0.1\n${grm_base}.g2_maf_lt_0.1\n${grm_base}.g3_maf_mt_0.1\n${grm_base}.g3_maf_lt_0.1\n${grm_base}.g4_maf_mt_0.1\n${grm_base}.g4_maf_lt_0.1 > $multiple_grm
# We then used GCTA to perform a GREML-LDMS analysis with the first 20 PCs calculated
# using HM3 SNPs from the WGS dataset fitted as fixed effects and the variants in the 8 MAF
# and LD bins as 8 random-effect components (Figure 1).

# --qcovar test.qcovar
# Input quantitative covariates from a plain text file, e.g. test.qcovar. Each quantitative covariate is recognized as a continuous variable.
# Input file format
# test.qcovar (no header line; columns are family ID, individual ID and quantitative covariates)

#PAPER
# In each imputed dataset, we stratified SNPs into 4 MAF bins (0.0001 < MAF < 0. 001,
# 0.001 < MAF < 0.01, 0.01 < MAF < 0.1, 0.1 < MAF < 0.5). For each of the 22 autosomes,
# we calculated the LD score of each variant with the others on a sliding window of 10Mb
# using GCTA software. We performed two types of LD binning, selecting variants based on
# their individual LD scores or on their segment-based LD scores (segment length=200Kb)
# (Supplementary Figure 9). Each of the 4 MAF bins was divided into 2 more bins, one for
# variants with LD scores above the median value of the variants in the bin (high LD bin)
# and one for variants with LD score below median (low LD bin) (Supplementary Table 5).
# We then used GCTA to perform a GREML-LDMS analysis with the first 20 PCs calculated
# using HM3 SNPs from the WGS dataset fitted as fixed effects and the variants in the 8 MAF
# and LD bins as 8 random-effect components (Figure 1).

multiple_grm=/home/edizadehm/03.resources/03.SCZ_CC/03.staging/multi_grm.txt
# $gcta --reml --reml-alg 2 --mgrm $multiple_grm --pheno $phen --prevalence 0.01 --out $greml --thread-num $thread --reml-maxit 10000